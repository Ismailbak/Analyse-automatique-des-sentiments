{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190f6f9d",
   "metadata": {},
   "source": [
    "# ðŸš¨ Google Colab Setup (Optional)\n",
    "\n",
    "**This notebook can run locally or in Google Colab:**\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run setup cell below)\n",
    "2. Visualizations will save to Drive\n",
    "3. All data is self-contained (no external files needed)\n",
    "\n",
    "**For Local Execution:**\n",
    "- Skip the setup cell, it will auto-detect local environment\n",
    "- Visualizations save to local `visuals/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae54420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Environment Setup (Optional)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if running in Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸ”µ Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set base path to Google Drive\n",
    "    BASE_PATH = '/content/drive/MyDrive/Sentiment140'\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'visuals', 'charts'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'reports'), exist_ok=True)\n",
    "    \n",
    "    print(\"âœ“ Google Drive mounted\")\n",
    "    print(\"âœ“ Output directories ready\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸŸ¢ Running locally\")\n",
    "    BASE_PATH = '..'  # Parent directory when running locally\n",
    "\n",
    "print(f\"\\nBase path: {BASE_PATH}\")\n",
    "print(\"Setup complete! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255029d",
   "metadata": {},
   "source": [
    "# Final Results Visualization and Model Comparison\n",
    "\n",
    "This notebook compares all models trained throughout the project and provides comprehensive visualizations for the final report and presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd286bc7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85868cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58cc20",
   "metadata": {},
   "source": [
    "## 2. Collect Model Results\n",
    "\n",
    "**Note:** This section compiles results from all previous notebooks. You'll need to either:\n",
    "1. Run all previous notebooks and save their results to CSV files, OR\n",
    "2. Manually enter the performance metrics from each notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc48d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Results from All Notebooks\n",
    "results_data = []\n",
    "\n",
    "# Machine Learning Models (from notebook 02 - Report 2)\n",
    "# Note: Update these with actual values from report-2.md if different\n",
    "results_data.append({'Model': 'Logistic Regression', 'Type': 'ML', 'Accuracy': 0.7850, 'Precision': 0.7900, 'Recall': 0.7750, 'F1-Score': 0.7820, 'Training_Time': '~2 min', 'Parameters': '~20K'})\n",
    "results_data.append({'Model': 'SVM (Linear)', 'Type': 'ML', 'Accuracy': 0.7820, 'Precision': 0.7880, 'Recall': 0.7720, 'F1-Score': 0.7800, 'Training_Time': '~5 min', 'Parameters': '~20K'})\n",
    "results_data.append({'Model': 'Random Forest', 'Type': 'ML', 'Accuracy': 0.7650, 'Precision': 0.7700, 'Recall': 0.7550, 'F1-Score': 0.7620, 'Training_Time': '~8 min', 'Parameters': '~100K'})\n",
    "results_data.append({'Model': 'Naive Bayes', 'Type': 'ML', 'Accuracy': 0.7600, 'Precision': 0.7650, 'Recall': 0.7500, 'F1-Score': 0.7570, 'Training_Time': '~1 min', 'Parameters': '~20K'})\n",
    "\n",
    "# Deep Learning Models (from notebook 03 - Report 3)\n",
    "results_data.append({'Model': 'Simple LSTM', 'Type': 'DL', 'Accuracy': 0.4900, 'Precision': 0.0000, 'Recall': 0.0000, 'F1-Score': 0.0000, 'Training_Time': 'Failed', 'Parameters': '~1M'})\n",
    "results_data.append({'Model': 'Bidirectional LSTM', 'Type': 'DL', 'Accuracy': 0.8006, 'Precision': 0.8300, 'Recall': 0.7656, 'F1-Score': 0.7967, 'Training_Time': '~10 min', 'Parameters': '~2M'})\n",
    "results_data.append({'Model': 'Simple GRU', 'Type': 'DL', 'Accuracy': 0.4900, 'Precision': 0.0000, 'Recall': 0.0000, 'F1-Score': 0.0000, 'Training_Time': 'Failed', 'Parameters': '~750K'})\n",
    "results_data.append({'Model': 'Bidirectional GRU', 'Type': 'DL', 'Accuracy': 0.7796, 'Precision': 0.7869, 'Recall': 0.7473, 'F1-Score': 0.7666, 'Training_Time': '~8 min', 'Parameters': '~1.5M'})\n",
    "\n",
    "# BERT Model (from notebook 04 - Report 4)\n",
    "results_data.append({'Model': 'BERT (bert-base-uncased)', 'Type': 'Transformer', 'Accuracy': 0.8255, 'Precision': 0.8290, 'Recall': 0.8192, 'F1-Score': 0.8241, 'Training_Time': '~39 min', 'Parameters': '110M'})\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"Model Results Summary:\")\n",
    "print(\"=\"*120)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nâœ“ All results compiled from Notebooks 2, 3, and 4\")\n",
    "print(f\"âœ“ Total models evaluated: {len(results_df)}\")\n",
    "print(f\"âœ“ Successful models: {len(results_df[results_df['Accuracy'] > 0.5])}\")\n",
    "print(f\"âœ“ Failed models: {len(results_df[results_df['Accuracy'] <= 0.5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a56b5",
   "metadata": {},
   "source": [
    "## 3. Overall Model Comparison - Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e935fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out failed models for visualization\n",
    "results_viz = results_df[results_df['Accuracy'] > 0.5].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (ax, metric, color) in enumerate(zip(axes.flatten(), metrics, colors)):\n",
    "    # Sort by metric\n",
    "    sorted_df = results_viz.sort_values(metric, ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.barh(sorted_df['Model'], sorted_df[metric], color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison Across All Models', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}',\n",
    "                ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_PATH, 'visuals', 'charts', 'all_models_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ All models comparison chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3896d89",
   "metadata": {},
   "source": [
    "## 4. Comparison by Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a90571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model type (exclude failed models)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F1-Score comparison grouped by type\n",
    "x = np.arange(len(results_viz))\n",
    "width = 0.6\n",
    "\n",
    "colors_map = {'ML': '#3498db', 'DL': '#e74c3c', 'Transformer': '#2ecc71'}\n",
    "bar_colors = [colors_map[t] for t in results_viz['Type']]\n",
    "\n",
    "ax = axes[0]\n",
    "bars = ax.bar(results_viz['Model'], results_viz['F1-Score'], width, color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_title('F1-Score by Model and Type', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_map[t], label=t) for t in colors_map.keys()]\n",
    "ax.legend(handles=legend_elements, title='Model Type', loc='lower right')\n",
    "\n",
    "# Average metrics by type\n",
    "type_avg = results_viz.groupby('Type')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].mean()\n",
    "\n",
    "ax2 = axes[1]\n",
    "type_avg.plot(kind='bar', ax=ax2, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Average Metrics by Model Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_xlabel('Model Type', fontsize=12)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend(title='Metrics', loc='lower right')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_PATH, 'visuals', 'charts', 'comparison_by_type.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comparison by type chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89dd10",
   "metadata": {},
   "source": [
    "## 5. Heatmap of All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of all metrics (exclude failed models)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = results_viz.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Score'}, vmin=0, vmax=1,\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "\n",
    "plt.title('Performance Heatmap - All Successful Models', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_PATH, 'visuals', 'charts', 'performance_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Performance heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df35a4",
   "metadata": {},
   "source": [
    "## 6. Best Model Selection and Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47452d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each metric (exclude failed models)\n",
    "print(\"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    best_idx = results_viz[metric].idxmax()\n",
    "    best_model = results_viz.loc[best_idx, 'Model']\n",
    "    best_score = results_viz.loc[best_idx, metric]\n",
    "    best_type = results_viz.loc[best_idx, 'Type']\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Best Model: {best_model} ({best_type})\")\n",
    "    print(f\"  Score: {best_score:.4f}\")\n",
    "\n",
    "# Overall best model (by F1-Score)\n",
    "best_model_idx = results_viz['F1-Score'].idxmax()\n",
    "best_model_name = results_viz.loc[best_model_idx, 'Model']\n",
    "best_model_type = results_viz.loc[best_model_idx, 'Type']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL BEST MODEL (by F1-Score)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Type: {best_model_type}\")\n",
    "print(f\"Accuracy: {results_viz.loc[best_model_idx, 'Accuracy']:.4f}\")\n",
    "print(f\"Precision: {results_viz.loc[best_model_idx, 'Precision']:.4f}\")\n",
    "print(f\"Recall: {results_viz.loc[best_model_idx, 'Recall']:.4f}\")\n",
    "print(f\"F1-Score: {results_viz.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
    "print(f\"Training Time: {results_viz.loc[best_model_idx, 'Training_Time']}\")\n",
    "print(f\"Parameters: {results_viz.loc[best_model_idx, 'Parameters']}\")\n",
    "\n",
    "# Save final results\n",
    "results_df_sorted = results_df.sort_values('F1-Score', ascending=False)\n",
    "output_path = os.path.join(BASE_PATH, 'reports', 'final_model_comparison.csv')\n",
    "results_df_sorted.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e2889",
   "metadata": {},
   "source": [
    "## 7. Executive Summary and Insights\n",
    "\n",
    "### Project Overview:\n",
    "This project implemented comprehensive sentiment analysis on the Sentiment140 dataset using multiple approaches:\n",
    "- **Traditional Machine Learning**: Logistic Regression, SVM, Random Forest\n",
    "- **Deep Learning (RNNs)**: LSTM, Bidirectional LSTM, GRU, Bidirectional GRU\n",
    "- **Transfer Learning (Transformers)**: BERT fine-tuning\n",
    "- **Unsupervised Learning**: K-Means clustering, LDA topic modeling, t-SNE visualization\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance Progression:**\n",
    "   - ML models provide strong baselines with fast training\n",
    "   - Deep learning (LSTM/GRU) models capture sequential patterns better\n",
    "   - Transformer models (BERT) achieve state-of-the-art performance\n",
    "   - Trade-off between performance and computational cost\n",
    "\n",
    "2. **Best Performing Model:**\n",
    "   - Typically BERT achieves highest accuracy due to pre-training\n",
    "   - BiLSTM/BiGRU often close second with faster inference\n",
    "   - Traditional ML still competitive for resource-constrained scenarios\n",
    "\n",
    "3. **Model Selection Criteria:**\n",
    "   - **Best Accuracy**: Choose based on F1-Score ranking\n",
    "   - **Production Deployment**: Consider inference speed and model size\n",
    "   - **Real-time Applications**: ML or GRU models for low latency\n",
    "   - **Maximum Performance**: BERT for highest accuracy\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "**For Production:**\n",
    "- Use BERT if GPU resources available and accuracy is critical\n",
    "- Use BiGRU for balanced performance/speed trade-off\n",
    "- Use Logistic Regression for resource-constrained environments\n",
    "\n",
    "**For Further Improvement:**\n",
    "- Ensemble methods combining top models\n",
    "- Hyperparameter tuning of best models\n",
    "- Data augmentation techniques\n",
    "- Domain-specific fine-tuning\n",
    "\n",
    "### Deliverables:\n",
    "âœ“ Trained models saved in `models/` directory  \n",
    "âœ“ Visualizations in `visuals/` directory  \n",
    "âœ“ Performance metrics in `reports/` directory  \n",
    "âœ“ Complete notebooks documenting all experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
