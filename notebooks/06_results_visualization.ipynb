{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4255029d",
   "metadata": {},
   "source": [
    "# Final Results Visualization and Model Comparison\n",
    "\n",
    "This notebook compares all models trained throughout the project and provides comprehensive visualizations for the final report and presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd286bc7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85868cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58cc20",
   "metadata": {},
   "source": [
    "## 2. Collect Model Results\n",
    "\n",
    "**Note:** This section compiles results from all previous notebooks. You'll need to either:\n",
    "1. Run all previous notebooks and save their results to CSV files, OR\n",
    "2. Manually enter the performance metrics from each notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc48d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load saved results, or create template for manual entry\n",
    "results_data = []\n",
    "\n",
    "# Example structure - Replace these with actual results after running notebooks\n",
    "# Machine Learning Models (from notebook 02)\n",
    "results_data.append({'Model': 'Logistic Regression', 'Type': 'ML', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "results_data.append({'Model': 'SVM', 'Type': 'ML', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "results_data.append({'Model': 'Random Forest', 'Type': 'ML', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "\n",
    "# Deep Learning Models (from notebook 03)\n",
    "results_data.append({'Model': 'LSTM', 'Type': 'DL', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "results_data.append({'Model': 'Bidirectional LSTM', 'Type': 'DL', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "results_data.append({'Model': 'GRU', 'Type': 'DL', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "results_data.append({'Model': 'Bidirectional GRU', 'Type': 'DL', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "\n",
    "# BERT Model (from notebook 04)\n",
    "results_data.append({'Model': 'BERT', 'Type': 'Transformer', 'Accuracy': 0.000, 'Precision': 0.000, 'Recall': 0.000, 'F1-Score': 0.000})\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"Model Results Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n⚠️  NOTE: Replace 0.000 values with actual results from previous notebooks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a56b5",
   "metadata": {},
   "source": [
    "## 3. Overall Model Comparison - Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e935fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (ax, metric, color) in enumerate(zip(axes.flatten(), metrics, colors)):\n",
    "    # Sort by metric\n",
    "    sorted_df = results_df.sort_values(metric, ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.barh(sorted_df['Model'], sorted_df[metric], color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison Across All Models', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}',\n",
    "                ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visuals/charts/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3896d89",
   "metadata": {},
   "source": [
    "## 4. Comparison by Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a90571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F1-Score comparison grouped by type\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.6\n",
    "\n",
    "colors_map = {'ML': '#3498db', 'DL': '#e74c3c', 'Transformer': '#2ecc71'}\n",
    "bar_colors = [colors_map[t] for t in results_df['Type']]\n",
    "\n",
    "ax = axes[0]\n",
    "bars = ax.bar(results_df['Model'], results_df['F1-Score'], width, color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_title('F1-Score by Model and Type', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_map[t], label=t) for t in colors_map.keys()]\n",
    "ax.legend(handles=legend_elements, title='Model Type', loc='lower right')\n",
    "\n",
    "# Average metrics by type\n",
    "type_avg = results_df.groupby('Type')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].mean()\n",
    "\n",
    "ax2 = axes[1]\n",
    "type_avg.plot(kind='bar', ax=ax2, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Average Metrics by Model Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_xlabel('Model Type', fontsize=12)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend(title='Metrics', loc='lower right')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visuals/charts/comparison_by_type.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89dd10",
   "metadata": {},
   "source": [
    "## 5. Heatmap of All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of all metrics\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Score'}, vmin=0, vmax=1,\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "\n",
    "plt.title('Performance Heatmap - All Models and Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visuals/charts/performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df35a4",
   "metadata": {},
   "source": [
    "## 6. Best Model Selection and Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47452d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each metric\n",
    "print(\"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    best_model = results_df.loc[best_idx, 'Model']\n",
    "    best_score = results_df.loc[best_idx, metric]\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Best Model: {best_model}\")\n",
    "    print(f\"  Score: {best_score:.4f}\")\n",
    "\n",
    "# Overall best model (by F1-Score)\n",
    "best_model_idx = results_df['F1-Score'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_model_type = results_df.loc[best_model_idx, 'Type']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL BEST MODEL (by F1-Score)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Type: {best_model_type}\")\n",
    "print(f\"Accuracy: {results_df.loc[best_model_idx, 'Accuracy']:.4f}\")\n",
    "print(f\"Precision: {results_df.loc[best_model_idx, 'Precision']:.4f}\")\n",
    "print(f\"Recall: {results_df.loc[best_model_idx, 'Recall']:.4f}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "# Save final results\n",
    "results_df_sorted = results_df.sort_values('F1-Score', ascending=False)\n",
    "results_df_sorted.to_csv('../reports/final_model_comparison.csv', index=False)\n",
    "print(\"\\n✓ Results saved to reports/final_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e2889",
   "metadata": {},
   "source": [
    "## 7. Executive Summary and Insights\n",
    "\n",
    "### Project Overview:\n",
    "This project implemented comprehensive sentiment analysis on the Sentiment140 dataset using multiple approaches:\n",
    "- **Traditional Machine Learning**: Logistic Regression, SVM, Random Forest\n",
    "- **Deep Learning (RNNs)**: LSTM, Bidirectional LSTM, GRU, Bidirectional GRU\n",
    "- **Transfer Learning (Transformers)**: BERT fine-tuning\n",
    "- **Unsupervised Learning**: K-Means clustering, LDA topic modeling, t-SNE visualization\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance Progression:**\n",
    "   - ML models provide strong baselines with fast training\n",
    "   - Deep learning (LSTM/GRU) models capture sequential patterns better\n",
    "   - Transformer models (BERT) achieve state-of-the-art performance\n",
    "   - Trade-off between performance and computational cost\n",
    "\n",
    "2. **Best Performing Model:**\n",
    "   - Typically BERT achieves highest accuracy due to pre-training\n",
    "   - BiLSTM/BiGRU often close second with faster inference\n",
    "   - Traditional ML still competitive for resource-constrained scenarios\n",
    "\n",
    "3. **Model Selection Criteria:**\n",
    "   - **Best Accuracy**: Choose based on F1-Score ranking\n",
    "   - **Production Deployment**: Consider inference speed and model size\n",
    "   - **Real-time Applications**: ML or GRU models for low latency\n",
    "   - **Maximum Performance**: BERT for highest accuracy\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "**For Production:**\n",
    "- Use BERT if GPU resources available and accuracy is critical\n",
    "- Use BiGRU for balanced performance/speed trade-off\n",
    "- Use Logistic Regression for resource-constrained environments\n",
    "\n",
    "**For Further Improvement:**\n",
    "- Ensemble methods combining top models\n",
    "- Hyperparameter tuning of best models\n",
    "- Data augmentation techniques\n",
    "- Domain-specific fine-tuning\n",
    "\n",
    "### Deliverables:\n",
    "✓ Trained models saved in `models/` directory  \n",
    "✓ Visualizations in `visuals/` directory  \n",
    "✓ Performance metrics in `reports/` directory  \n",
    "✓ Complete notebooks documenting all experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
