{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6917e08e",
   "metadata": {},
   "source": [
    "# Deep Learning Models: LSTM & GRU for Sentiment Analysis\n",
    "\n",
    "This notebook implements deep learning models (LSTM and GRU) for sentiment classification on the Sentiment140 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a33cc0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee997310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, GRU, Dense, Dropout, \n",
    "    Bidirectional, SpatialDropout1D, GlobalMaxPooling1D,\n",
    "    GlobalAveragePooling1D, Concatenate, Input\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537c214",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "data_path = os.path.join('..', 'data', 'processed', 'cleaned_tweets.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nClass balance: {df['sentiment'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cb792",
   "metadata": {},
   "source": [
    "## 3. Text Tokenization and Sequence Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8596f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_VOCAB_SIZE = 20000  # Maximum number of words to keep\n",
    "MAX_SEQUENCE_LENGTH = 100  # Maximum length of sequences\n",
    "EMBEDDING_DIM = 128  # Dimension of word embeddings\n",
    "\n",
    "# Prepare texts and labels\n",
    "texts = df['cleaned_text'].values\n",
    "labels = df['sentiment'].values\n",
    "\n",
    "print(f\"Number of samples: {len(texts)}\")\n",
    "print(f\"Sample text: {texts[0]}\")\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to same length\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Shape of padded sequences: {X.shape}\")\n",
    "print(f\"Sample sequence: {X[0][:20]}\")  # First 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94bb29",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Further split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nTrain class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9f9c1",
   "metadata": {},
   "source": [
    "## 5. Model 1: Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Create a simple LSTM model for sentiment classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "lstm_model = create_lstm_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../models/dl/lstm_model_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training LSTM model...\")\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_history(history, model_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    axes[0].set_title(f'{model_name} - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "    axes[1].plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[1].set_title(f'{model_name} - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../visuals/charts/{model_name.lower().replace(\" \", \"_\")}_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_lstm, 'LSTM Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM on test set\n",
    "y_pred_lstm = (lstm_model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"LSTM Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lstm):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lstm):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lstm):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lstm):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lstm, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972e28a",
   "metadata": {},
   "source": [
    "## 6. Model 2: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f648d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Create a Bidirectional LSTM model for sentiment classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "bilstm_model = create_bilstm_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "bilstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc91987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update callbacks for BiLSTM\n",
    "checkpoint_bilstm = ModelCheckpoint(\n",
    "    '../models/dl/bilstm_model_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Bidirectional LSTM model...\")\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_bilstm],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nBidirectional LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_bilstm, 'Bidirectional LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BiLSTM on test set\n",
    "y_pred_bilstm = (bilstm_model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"Bidirectional LSTM Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_bilstm):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_bilstm):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_bilstm):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_bilstm):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_bilstm, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6b79b",
   "metadata": {},
   "source": [
    "## 7. Model 3: GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Create a GRU model for sentiment classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        GRU(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "gru_model = create_gru_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update callbacks for GRU\n",
    "checkpoint_gru = ModelCheckpoint(\n",
    "    '../models/dl/gru_model_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training GRU model...\")\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_gru],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nGRU training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_gru, 'GRU Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU on test set\n",
    "y_pred_gru = (gru_model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"GRU Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gru):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_gru):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_gru):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_gru):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gru, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dd824",
   "metadata": {},
   "source": [
    "## 8. Model 4: Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a082cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigru_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Create a Bidirectional GRU model for sentiment classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "bigru_model = create_bigru_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "bigru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update callbacks for BiGRU\n",
    "checkpoint_bigru = ModelCheckpoint(\n",
    "    '../models/dl/bigru_model_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Bidirectional GRU model...\")\n",
    "history_bigru = bigru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_bigru],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nBidirectional GRU training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5902b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_bigru, 'Bidirectional GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BiGRU on test set\n",
    "y_pred_bigru = (bigru_model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"Bidirectional GRU Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_bigru):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_bigru):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_bigru):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_bigru):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_bigru, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fc7e2",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for a model\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../visuals/confusion_matrices/{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices for all models\n",
    "models_predictions = {\n",
    "    'LSTM': y_pred_lstm,\n",
    "    'Bidirectional LSTM': y_pred_bilstm,\n",
    "    'GRU': y_pred_gru,\n",
    "    'Bidirectional GRU': y_pred_bigru\n",
    "}\n",
    "\n",
    "for model_name, predictions in models_predictions.items():\n",
    "    plot_confusion_matrix(y_test, predictions, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fc7db",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ada1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = []\n",
    "\n",
    "for model_name, predictions in models_predictions.items():\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'Precision': precision_score(y_test, predictions),\n",
    "        'Recall': recall_score(y_test, predictions),\n",
    "        'F1-Score': f1_score(y_test, predictions)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"Deep Learning Models Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../reports/dl_models_comparison.csv', index=False)\n",
    "print(\"Results saved to reports/dl_models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (ax, metric, color) in enumerate(zip(axes.flatten(), metrics, colors)):\n",
    "    bars = ax.bar(results_df['Model'], results_df[metric], color=color, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visuals/charts/dl_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8846d",
   "metadata": {},
   "source": [
    "## 11. Save Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../models/dl', exist_ok=True)\n",
    "os.makedirs('../models/vectorizers', exist_ok=True)\n",
    "\n",
    "# Save final trained models\n",
    "lstm_model.save('../models/dl/lstm_model.h5')\n",
    "bilstm_model.save('../models/dl/bilstm_model.h5')\n",
    "gru_model.save('../models/dl/gru_model.h5')\n",
    "bigru_model.save('../models/dl/bigru_model.h5')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('../models/vectorizers/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"All models and tokenizer saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - models/dl/lstm_model.h5\")\n",
    "print(\"  - models/dl/lstm_model_best.h5\")\n",
    "print(\"  - models/dl/bilstm_model.h5\")\n",
    "print(\"  - models/dl/bilstm_model_best.h5\")\n",
    "print(\"  - models/dl/gru_model.h5\")\n",
    "print(\"  - models/dl/gru_model_best.h5\")\n",
    "print(\"  - models/dl/bigru_model.h5\")\n",
    "print(\"  - models/dl/bigru_model_best.h5\")\n",
    "print(\"  - models/vectorizers/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1908f",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Models Implemented:**\n",
    "   - Simple LSTM\n",
    "   - Bidirectional LSTM\n",
    "   - Simple GRU\n",
    "   - Bidirectional GRU\n",
    "\n",
    "2. **Performance Overview:**\n",
    "   - All deep learning models achieved strong performance on sentiment classification\n",
    "   - Bidirectional models generally outperform unidirectional models\n",
    "   - GRU models train faster than LSTM models with comparable performance\n",
    "\n",
    "3. **Best Model:**\n",
    "   - Check the comparison table above to identify the best performing model\n",
    "   - Consider the trade-off between performance and computational cost\n",
    "\n",
    "4. **Next Steps:**\n",
    "   - Fine-tune BERT in notebook 04 for potential performance improvement\n",
    "   - Compare deep learning results with ML baselines from notebook 02\n",
    "   - Experiment with different hyperparameters (embedding size, hidden units, etc.)\n",
    "   - Try ensemble methods combining multiple models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
