{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3fd249",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a0518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc4a67",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1092947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (99735, 4)\n",
      "\n",
      "Columns: ['text', 'text_clean', 'sentiment', 'sentiment_label']\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "0    49893\n",
      "1    49842\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "      <td>oh no where did u order from that s horrible</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "      <td>a great hard training weekend is over a couple...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "      <td>right off to work only hours to go until i m f...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am craving for japanese food</td>\n",
       "      <td>i am craving for japanese food</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "      <td>jean michel jarre concert tomorrow gotta work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @xnausikaax oh no! where did u order from? tha...   \n",
       "1  A great hard training weekend is over.  a coup...   \n",
       "2  Right, off to work  Only 5 hours to go until I...   \n",
       "3                    I am craving for japanese food    \n",
       "4  Jean Michel Jarre concert tomorrow  gotta work...   \n",
       "\n",
       "                                          text_clean  sentiment  \\\n",
       "0       oh no where did u order from that s horrible          0   \n",
       "1  a great hard training weekend is over a couple...          0   \n",
       "2  right off to work only hours to go until i m f...          0   \n",
       "3                     i am craving for japanese food          0   \n",
       "4  jean michel jarre concert tomorrow gotta work ...          0   \n",
       "\n",
       "  sentiment_label  \n",
       "0        negative  \n",
       "1        negative  \n",
       "2        negative  \n",
       "3        negative  \n",
       "4        negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path('../data/processed/cleaned_tweets.csv')\n",
    "MODELS_PATH = Path('../models/ml')\n",
    "VECTORIZER_PATH = Path('../models/vectorizers')\n",
    "VISUALS_PATH = Path('../visuals/confusion_matrices')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "VECTORIZER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "VISUALS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c65863",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8eafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (99735,)\n",
      "Labels shape: (99735,)\n",
      "\n",
      "Label distribution:\n",
      "sentiment\n",
      "0    49893\n",
      "1    49842\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract features (cleaned text) and labels (sentiment)\n",
    "X = df['text_clean']  # Cleaned text\n",
    "y = df['sentiment']   # 0 = negative, 1 = positive\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277573cf",
   "metadata": {},
   "source": [
    "## 4. Split Data: Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e312243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 79788 samples\n",
      "Testing set size: 19947 samples\n",
      "\n",
      "Training set sentiment distribution:\n",
      "sentiment\n",
      "0    39914\n",
      "1    39874\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing set sentiment distribution:\n",
      "sentiment\n",
      "0    9979\n",
      "1    9968\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data into training (80%) and testing (20%)\n",
    "# stratify=y ensures both sets have the same proportion of positive/negative sentiments\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y          # Maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(f\"\\nTraining set sentiment distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set sentiment distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cf531",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) converts text into numerical features:\n",
    "- **TF**: How often a word appears in a document\n",
    "- **IDF**: How rare/important a word is across all documents\n",
    "- Common words (like \"the\", \"is\") get low scores\n",
    "- Rare, meaningful words get high scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1314a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to TF-IDF features...\n",
      "This may take a few minutes...\n",
      "\n",
      "‚úì Vectorization completed in 2.83 seconds\n",
      "\n",
      "Training features shape: (79788, 10000)\n",
      "Testing features shape: (19947, 10000)\n",
      "\n",
      "Vocabulary size: 10000 words\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# max_features=10000: Keep only the 10,000 most important words\n",
    "# min_df=5: Ignore words that appear in fewer than 5 documents\n",
    "# max_df=0.7: Ignore words that appear in more than 70% of documents (too common)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    ngram_range=(1, 2)  # Use single words (unigrams) and word pairs (bigrams)\n",
    ")\n",
    "\n",
    "print(\"Converting text to TF-IDF features...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Vectorization completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\\nTraining features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing features shape: {X_test_tfidf.shape}\")\n",
    "print(f\"\\nVocabulary size: {len(tfidf.vocabulary_)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab856011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TF-IDF vectorizer saved to: ..\\models\\vectorizers\\tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the TF-IDF vectorizer for later use\n",
    "vectorizer_file = VECTORIZER_PATH / 'tfidf_vectorizer.pkl'\n",
    "with open(vectorizer_file, 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(f\"‚úì TF-IDF vectorizer saved to: {vectorizer_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889565b6",
   "metadata": {},
   "source": [
    "## 6. Model 1: Logistic Regression\n",
    "\n",
    "**Logistic Regression** is a simple, fast linear model:\n",
    "- Good baseline for binary classification\n",
    "- Works well with high-dimensional sparse data (like TF-IDF)\n",
    "- Fast to train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6782ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING LOGISTIC REGRESSION\n",
      "============================================================\n",
      "\n",
      "‚úì Training completed in 15.28 seconds\n",
      "\n",
      "Model trained on 79788 samples\n",
      "Predictions made on 19947 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train Logistic Regression model\n",
    "# max_iter=1000: Maximum number of iterations\n",
    "# C=1.0: Regularization strength (smaller = stronger regularization)\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Training completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\\nModel trained on {len(X_train)} samples\")\n",
    "print(f\"Predictions made on {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5448039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOGISTIC REGRESSION RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.7919 (79.19%)\n",
      "Precision: 0.7882\n",
      "Recall:    0.7980\n",
      "F1-Score:  0.7931\n",
      "\n",
      "============================================================\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.79      0.79      9979\n",
      "    Positive       0.79      0.80      0.79      9968\n",
      "\n",
      "    accuracy                           0.79     19947\n",
      "   macro avg       0.79      0.79      0.79     19947\n",
      "weighted avg       0.79      0.79      0.79     19947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall:    {lr_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lr_f1:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce25532",
   "metadata": {},
   "source": [
    "## 7. Model 2: Support Vector Machine (SVM)\n",
    "\n",
    "**LinearSVC** (Linear Support Vector Classification):\n",
    "- Finds the best hyperplane to separate classes\n",
    "- Often performs very well on text classification\n",
    "- Good with high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668d0a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SUPPORT VECTOR MACHINE (SVM)\n",
      "============================================================\n",
      "\n",
      "‚úì Training completed in 1.39 seconds\n",
      "\n",
      "Model trained on 79788 samples\n",
      "Predictions made on 19947 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUPPORT VECTOR MACHINE (SVM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train SVM model\n",
    "# C=1.0: Regularization parameter\n",
    "# max_iter=1000: Maximum iterations\n",
    "svm_model = LinearSVC(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Training completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\\nModel trained on {len(X_train)} samples\")\n",
    "print(f\"Predictions made on {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_precision = precision_score(y_test, y_pred_svm)\n",
    "svm_recall = recall_score(y_test, y_pred_svm)\n",
    "svm_f1 = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUPPORT VECTOR MACHINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {svm_precision:.4f}\")\n",
    "print(f\"Recall:    {svm_recall:.4f}\")\n",
    "print(f\"F1-Score:  {svm_f1:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b147e",
   "metadata": {},
   "source": [
    "## 8. Model 3: Random Forest\n",
    "\n",
    "**Random Forest**:\n",
    "- Ensemble of decision trees\n",
    "- Each tree votes on the final prediction\n",
    "- Good at capturing non-linear patterns\n",
    "- More robust to overfitting than single decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train Random Forest model\n",
    "# n_estimators=100: Number of trees in the forest\n",
    "# max_depth=20: Maximum depth of each tree\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1   # Show progress\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Training completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\\nModel trained on {len(X_train)} samples\")\n",
    "print(f\"Predictions made on {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59362724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall:    {rf_recall:.4f}\")\n",
    "print(f\"F1-Score:  {rf_f1:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc31af9",
   "metadata": {},
   "source": [
    "## 9. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0813eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'SVM (LinearSVC)', 'Random Forest'],\n",
    "    'Accuracy': [lr_accuracy, svm_accuracy, rf_accuracy],\n",
    "    'Precision': [lr_precision, svm_precision, rf_precision],\n",
    "    'Recall': [lr_recall, svm_recall, rf_recall],\n",
    "    'F1-Score': [lr_f1, svm_f1, rf_f1]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "results = results.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "best_f1 = results.iloc[0]['F1-Score']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1-Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(results['Model'], results[metric], color=colors)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(results['Model'], rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visuals/charts/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Comparison chart saved to visuals/charts/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c2280",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrices\n",
    "\n",
    "**Confusion Matrix** shows:\n",
    "- **True Positives (TP)**: Correctly predicted positive\n",
    "- **True Negatives (TN)**: Correctly predicted negative\n",
    "- **False Positives (FP)**: Predicted positive, actually negative\n",
    "- **False Negatives (FN)**: Predicted negative, actually positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix for a model.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Add accuracy in the plot\n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\n",
    "    plt.text(0.5, -0.15, f'Accuracy: {accuracy:.4f}', \n",
    "             ha='center', transform=plt.gca().transAxes, fontsize=11)\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f\"../visuals/confusion_matrices/{model_name.lower().replace(' ', '_')}_cm.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Confusion matrix saved to {filename}\")\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "print(\"Generating confusion matrices...\\n\")\n",
    "\n",
    "cm_lr = plot_confusion_matrix(y_test, y_pred_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_svm = plot_confusion_matrix(y_test, y_pred_svm, 'SVM LinearSVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d837ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = plot_confusion_matrix(y_test, y_pred_rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5d739",
   "metadata": {},
   "source": [
    "## 11. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which model is best based on F1-Score\n",
    "models_dict = {\n",
    "    'Logistic Regression': (lr_model, lr_f1),\n",
    "    'SVM (LinearSVC)': (svm_model, svm_f1),\n",
    "    'Random Forest': (rf_model, rf_f1)\n",
    "}\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(models_dict, key=lambda k: models_dict[k][1])\n",
    "best_model = models_dict[best_model_name][0]\n",
    "best_score = models_dict[best_model_name][1]\n",
    "\n",
    "# Save all models\n",
    "print(\"Saving models...\\n\")\n",
    "\n",
    "for model_name, (model, score) in models_dict.items():\n",
    "    filename = MODELS_PATH / f\"{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"‚úì {model_name} saved to: {filename}\")\n",
    "\n",
    "# Also save best model separately\n",
    "best_model_file = MODELS_PATH / 'best_model.pkl'\n",
    "with open(best_model_file, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"\\nüèÜ Best model ({best_model_name}) saved to: {best_model_file}\")\n",
    "print(f\"   F1-Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3de8f9",
   "metadata": {},
   "source": [
    "## 12. Test on Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model on new sample texts\n",
    "sample_texts = [\n",
    "    \"i love this product it is amazing\",\n",
    "    \"this is the worst experience ever\",\n",
    "    \"absolutely fantastic cant wait to use it again\",\n",
    "    \"terrible service very disappointed\",\n",
    "    \"great quality highly recommend\",\n",
    "    \"waste of money do not buy\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING ON SAMPLE TEXTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Transform sample texts using TF-IDF\n",
    "sample_tfidf = tfidf.transform(sample_texts)\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(sample_tfidf)\n",
    "\n",
    "# Display results\n",
    "for text, pred in zip(sample_texts, predictions):\n",
    "    sentiment = \"üòä POSITIVE\" if pred == 1 else \"üòû NEGATIVE\"\n",
    "    print(f\"\\nText: \\\"{text}\\\"\")\n",
    "    print(f\"Prediction: {sentiment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946449a",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Dataset:**\n",
    "- Training samples: 80,000 tweets\n",
    "- Testing samples: 20,000 tweets\n",
    "- Features: TF-IDF with 10,000 words\n",
    "\n",
    "**Models Trained:**\n",
    "1. Logistic Regression - Fast, simple baseline\n",
    "2. SVM (LinearSVC) - Good for text classification\n",
    "3. Random Forest - Ensemble method\n",
    "\n",
    "**Next Steps (Week 4):**\n",
    "- Implement Deep Learning models (RNN, LSTM, GRU)\n",
    "- Compare DL vs ML performance\n",
    "- Try word embeddings (Word2Vec, GloVe)\n",
    "\n",
    "---\n",
    "\n",
    "‚úì Notebook 02 Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4af400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK 02 SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì Dataset loaded: {len(df)} tweets\")\n",
    "print(f\"‚úì Train/Test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "print(f\"‚úì TF-IDF features: {X_train_tfidf.shape[1]} dimensions\")\n",
    "print(f\"\\n‚úì Models trained: 3 (Logistic Regression, SVM, Random Forest)\")\n",
    "print(f\"‚úì Best model: {best_model_name}\")\n",
    "print(f\"‚úì Best F1-Score: {best_score:.4f}\")\n",
    "print(f\"\\n‚úì Models saved to: {MODELS_PATH}\")\n",
    "print(f\"‚úì Visualizations saved to: {VISUALS_PATH}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready for Week 4: Deep Learning Models!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
