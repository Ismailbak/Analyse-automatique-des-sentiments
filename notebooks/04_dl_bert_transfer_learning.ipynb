{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31aa4271",
   "metadata": {},
   "source": [
    "# ðŸš€ Google Colab Setup\n",
    "\n",
    "**Before running this notebook on Colab:**\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU (T4 or better)\n",
    "2. **Mount Google Drive**: Run the cell below to access your data files\n",
    "3. **Upload your data**: Make sure `cleaned_tweets.csv` is in your Drive at the specified path\n",
    "\n",
    "**âš ï¸ IMPORTANT: BERT requires GPU for reasonable training time!**\n",
    "- With GPU (T4): ~30-60 minutes total training\n",
    "- Without GPU: 6-12 hours total training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ“ Running on Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive to access your data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set base path - ADJUST THIS to match your Drive folder structure\n",
    "    BASE_PATH = '/content/drive/MyDrive/Sentiment140'\n",
    "    \n",
    "    # Check GPU availability - CRITICAL FOR BERT!\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ“ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(\"âœ“ BERT training will be fast!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ WARNING: No GPU detected! BERT training will be VERY SLOW.\")\n",
    "        print(\"   Please enable GPU: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "    \n",
    "    # Install required packages\n",
    "    print(\"\\nðŸ“¦ Installing required packages...\")\n",
    "    !pip install -q transformers torch\n",
    "    \n",
    "    # Create necessary directories in Drive if they don't exist\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'models', 'dl', 'bert_model'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'visuals', 'charts'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'visuals', 'confusion_matrices'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_PATH, 'reports'), exist_ok=True)\n",
    "    \n",
    "    print(\"âœ“ All directories ready!\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    BASE_PATH = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5e4d2",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning for Sentiment Analysis\n",
    "\n",
    "This notebook implements transfer learning using pre-trained BERT (Bidirectional Encoder Representations from Transformers) for sentiment classification on the Sentiment140 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc08bc",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Add src to path (only for local, not needed in Colab)\n",
    "if 'BASE_PATH' not in dir():\n",
    "    BASE_PATH = '..'\n",
    "sys.path.append(os.path.join(BASE_PATH, 'src'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ad1ad",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f72c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "data_path = os.path.join(BASE_PATH, 'data', 'processed', 'cleaned_tweets.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nSample tweets:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT, we'll use a smaller sample for faster training\n",
    "# You can increase this if you have sufficient GPU memory\n",
    "SAMPLE_SIZE = 50000  # Adjust based on your hardware capabilities\n",
    "\n",
    "# Sample data if dataset is large\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"\\nUsing sample of {SAMPLE_SIZE} tweets for faster training\")\n",
    "else:\n",
    "    df_sample = df\n",
    "    print(f\"\\nUsing full dataset of {len(df)} tweets\")\n",
    "\n",
    "# Prepare texts and labels\n",
    "texts = df_sample['text_clean'].values\n",
    "labels = df_sample['sentiment'].values\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(texts)}\")\n",
    "print(f\"Label distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd9881",
   "metadata": {},
   "source": [
    "## 3. Load BERT Tokenizer and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)\n",
    "\n",
    "print(f\"Loaded tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nTokenizer special tokens:\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  CLS token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"  SEP token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2350759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "MAX_LENGTH = 128  # Maximum sequence length for BERT\n",
    "\n",
    "# Tokenize all texts\n",
    "print(\"Tokenizing texts...\")\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts.tolist(),\n",
    "    add_special_tokens=True,        # Add [CLS] and [SEP] tokens\n",
    "    max_length=MAX_LENGTH,           # Max length to pad/truncate\n",
    "    padding='max_length',            # Pad to max_length\n",
    "    truncation=True,                 # Truncate longer sequences\n",
    "    return_attention_mask=True,      # Return attention masks\n",
    "    return_tensors='pt'              # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "print(f\"\\nTokenization complete!\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Labels shape: {labels_tensor.shape}\")\n",
    "print(f\"\\nExample tokenized sequence:\")\n",
    "print(f\"Text: {texts[0][:100]}...\")\n",
    "print(f\"Token IDs: {input_ids[0][:20]}\")  # First 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1066a",
   "metadata": {},
   "source": [
    "## 4. Create Train/Validation/Test DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f07a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "train_val_inputs, test_inputs, train_val_labels, test_labels, train_val_masks, test_masks = train_test_split(\n",
    "    input_ids, labels_tensor, attention_masks, test_size=0.2, random_state=42, stratify=labels_tensor\n",
    ")\n",
    "\n",
    "# Second split: separate validation set from training (10% of original, ~12.5% of remaining)\n",
    "train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(\n",
    "    train_val_inputs, train_val_labels, train_val_masks, test_size=0.125, random_state=42, stratify=train_val_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_inputs)}\")\n",
    "print(f\"Validation samples: {len(val_inputs)}\")\n",
    "print(f\"Test samples: {len(test_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created!\")\n",
    "print(f\"  Training batches: {len(train_dataloader)}\")\n",
    "print(f\"  Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"  Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dda1a",
   "metadata": {},
   "source": [
    "## 5. Load Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634affbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model with sequence classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,           # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Loaded model: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f204fe",
   "metadata": {},
   "source": [
    "## 6. Setup Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a467a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 3  # Typical for BERT fine-tuning\n",
    "LEARNING_RATE = 2e-5  # Recommended for BERT\n",
    "WARMUP_STEPS = 0\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=EPSILON\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09f586",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Format time in hh:mm:ss\"\"\"\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training history\n",
    "training_stats = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Training phase\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 100 batches\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f\"  Batch {step:>5,} of {len(train_dataloader):>5,}.    Elapsed: {elapsed}.\")\n",
    "        \n",
    "        # Unpack batch and move to GPU\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Training epoch took: {training_time}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    print(f\"\\nRunning Validation...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        total_eval_accuracy += np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Validation took: {validation_time}\")\n",
    "    \n",
    "    # Record stats\n",
    "    training_stats.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'Training Loss': avg_train_loss,\n",
    "        'Valid. Loss': avg_val_loss,\n",
    "        'Valid. Accur.': avg_val_accuracy,\n",
    "        'Training Time': training_time,\n",
    "        'Validation Time': validation_time\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Total training took {format_time(time.time() - total_t0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training statistics\n",
    "training_stats_df = pd.DataFrame(training_stats)\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(training_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be65c6f",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "axes[0].plot(training_stats_df['epoch'], training_stats_df['Training Loss'], \n",
    "             marker='o', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(training_stats_df['epoch'], training_stats_df['Valid. Loss'], \n",
    "             marker='s', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation accuracy\n",
    "axes[1].plot(training_stats_df['epoch'], training_stats_df['Valid. Accur.'], \n",
    "             marker='o', color='green', linewidth=2)\n",
    "axes[1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_PATH, 'visuals', 'charts', 'bert_training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e87ce8",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_input_mask\n",
    "        )\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "    true_labels.extend(label_ids.flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "print(\"\\nBERT Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, predictions):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, predictions):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(true_labels, predictions):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8dbbd7",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d65c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - BERT Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_PATH, 'visuals', 'confusion_matrices', 'bert_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff43038",
   "metadata": {},
   "source": [
    "## 11. Save BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a82ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "output_dir = os.path.join(BASE_PATH, 'models', 'dl', 'bert_model')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\nBERT model and tokenizer saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  from transformers import BertForSequenceClassification, BertTokenizer\")\n",
    "print(f\"  model = BertForSequenceClassification.from_pretrained('{output_dir}')\")\n",
    "print(f\"  tokenizer = BertTokenizer.from_pretrained('{output_dir}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8916b",
   "metadata": {},
   "source": [
    "## 12. Test Predictions on Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        probabilities = torch.softmax(logits, dim=1)[0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = probabilities[prediction].item()\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test on sample texts\n",
    "test_samples = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst experience ever. Very disappointed.\",\n",
    "    \"The movie was okay, nothing special.\",\n",
    "    \"Fantastic service and great quality!\",\n",
    "    \"I hate waiting in long lines.\",\n",
    "    \"Best day of my life!\",\n",
    "    \"Terrible customer support, never buying again.\"\n",
    "]\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*70)\n",
    "for text in test_samples:\n",
    "    sentiment, confidence = predict_sentiment(text, model, tokenizer, device)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba00ec7",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Architecture:**\n",
    "   - Pre-trained BERT (bert-base-uncased) with 110M parameters\n",
    "   - Fine-tuned for binary sentiment classification\n",
    "   - Used transfer learning to leverage pre-trained knowledge\n",
    "\n",
    "2. **Training Configuration:**\n",
    "   - 3 epochs (typical for BERT fine-tuning)\n",
    "   - Learning rate: 2e-5 (recommended for BERT)\n",
    "   - Batch size: 16\n",
    "   - Max sequence length: 128 tokens\n",
    "\n",
    "3. **Performance:**\n",
    "   - Check the evaluation metrics above\n",
    "   - BERT typically achieves higher accuracy than LSTM/GRU models\n",
    "   - Benefits from contextual embeddings and bidirectional attention\n",
    "\n",
    "4. **Advantages of BERT:**\n",
    "   - Pre-trained on massive text corpus\n",
    "   - Understands context better than sequential models\n",
    "   - Captures semantic relationships effectively\n",
    "   - State-of-the-art performance on NLP tasks\n",
    "\n",
    "5. **Considerations:**\n",
    "   - Requires more computational resources (GPU recommended)\n",
    "   - Slower training compared to LSTM/GRU\n",
    "   - Larger model size\n",
    "   - Trade-off between performance and efficiency\n",
    "\n",
    "### Next Steps:\n",
    "- Compare BERT performance with ML models (notebook 02) and LSTM/GRU (notebook 03)\n",
    "- Experiment with different BERT variants (RoBERTa, DistilBERT, etc.)\n",
    "- Try ensemble methods combining multiple models\n",
    "- Perform error analysis to understand misclassifications"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
